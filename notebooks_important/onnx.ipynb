{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для конвертации модели в формат ONNX с помощью PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "\n",
    "def convert_from_torch_to_onnx(\n",
    "        onnx_path: str,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        model: AutoModelForSequenceClassification\n",
    ") -> None:\n",
    "    \"\"\"Конвертация модели из формата PyTorch в формат ONNX.\n",
    "\n",
    "    @param onnx_path: путь к модели в формате ONNX\n",
    "    @param tokenizer: токенизатор\n",
    "    @param model: модель\n",
    "    \"\"\"\n",
    "    dummy_model_input = tokenizer(\n",
    "        \"текст для конвертации\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cpu\")\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_model_input[\"input_ids\"],\n",
    "        onnx_path,\n",
    "        opset_version=12,\n",
    "        input_names=[\"input_ids\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\n",
    "            \"input_ids\": {\n",
    "                0: \"batch_size\",\n",
    "                1: \"sequence_len\"\n",
    "            },\n",
    "            \"output\": {\n",
    "                0: \"batch_size\"\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для квантизации модели в формате ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import (\n",
    "    quantize_dynamic,\n",
    "    QuantType\n",
    ")\n",
    "\n",
    "\n",
    "def convert_from_onnx_to_quantized_onnx(\n",
    "        onnx_model_path: str,\n",
    "        quantized_onnx_model_path: str\n",
    ") -> None:\n",
    "    \"\"\"Квантизация модели в формате ONNX до Int8\n",
    "    и сохранение кванитизированной модели на диск.\n",
    "\n",
    "    @param onnx_model_path: путь к модели в формате ONNX\n",
    "    @param quantized_onnx_model_path: путь к квантизированной модели\n",
    "    \"\"\"\n",
    "    quantize_dynamic(\n",
    "        onnx_model_path,\n",
    "        quantized_onnx_model_path,\n",
    "        weight_type=QuantType.QUInt8\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для инференса модели c с помощью PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "def pytorch_inference(\n",
    "        text: str,\n",
    "        max_tokens: int,\n",
    "        model: AutoModelForSequenceClassification,\n",
    "        tokenizer: AutoTokenizer,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Инференс модели с помощью PyTorch.\n",
    "\n",
    "    @param text: входной текст для классификации\n",
    "    @param max_tokens: максимальная длина последовательности в токенах\n",
    "    @param model: BERT-модель\n",
    "    @param tokenizer: токенизатор\n",
    "    @return: логиты на выходе из модели\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_tokens,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to('cpu')\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**inputs).logits.detach()\n",
    "    return outputs\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для создания сессии ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "from onnxruntime import (\n",
    "    InferenceSession,\n",
    "    SessionOptions\n",
    ")\n",
    "\n",
    "\n",
    "def create_onnx_session(\n",
    "        model_path: str,\n",
    "        provider: str = \"CPUExecutionProvider\"\n",
    ") -> InferenceSession:\n",
    "    \"\"\"Создание сессии для инференса модели с помощью ONNX Runtime.\n",
    "\n",
    "    @param model_path: путь к модели в формате ONNX\n",
    "    @param provider: инференс на ЦП\n",
    "    @return: ONNX Runtime-сессия\n",
    "    \"\"\"\n",
    "    options = SessionOptions()\n",
    "    options.graph_optimization_level = \\\n",
    "        onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    options.intra_op_num_threads = 1\n",
    "    session = InferenceSession(model_path, options, providers=[provider])\n",
    "    session.disable_fallback()\n",
    "    return session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для инференса модели c помощью ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from onnxruntime import InferenceSession\n",
    "\n",
    "\n",
    "def onnx_inference(\n",
    "        text: str,\n",
    "        session: InferenceSession,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_length: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Инференс модели с помощью ONNX Runtime.\n",
    "\n",
    "    @param text: входной текст для классификации\n",
    "    @param session: ONNX Runtime-сессия\n",
    "    @param tokenizer: токенизатор\n",
    "    @param max_length: максимальная длина последовательности в токенах\n",
    "    @return: логиты на выходе из модели\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    input_feed = {\n",
    "        \"input_ids\": inputs[\"input_ids\"].astype(np.int64)\n",
    "    }\n",
    "    outputs = session.run(\n",
    "        output_names=[\"output\"],\n",
    "        input_feed=input_feed\n",
    "    )[0]\n",
    "    return outputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
